# -*- coding: utf-8 -*-
"""ElfajarBintangSamudera_VIX_ID/X Partners.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15kcyBOvECpwAA-2Ffp1RjtjXa78-zOFA

# RAKAMIN ACADEMY

## Nama : Elfajar Bintang Samudera
## Task : End to End Solution (WEEK 04)

### Tugas yang perlu diselesaikan

1. File dataset yang disediakan terdiri dari satu file data (format csv) dan satu data dictionary (format xlsx) yang berisikan informasi mengenai masing-masing kolom yang ada pada file data. Silahkan pelajari informasi mengenai data tersebut untuk mengetahui langkah apa yang perlu dilakukan untuk mempersiapkan data
2. Siapkan tools yang akan kamu gunakan untuk membantu pengerjaan soal seperti
yang telah dijelaskan di prerequisite, lalu import data untuk mempersiapkan
pengerjaan.
3. Terdapat dua skenario pengumpulan tugas berdasarkan bahasa pemrograman yang kamu gunakan untuk mengerjakan final project ini.

  a. Apabila kamu menggunakan R untuk mengerjakan tugas ini, kamu perlu mengumpulkan:
    - File Code dengan format .R
    - File R Markdown dengan format .Rmd
    - File infografis sebagai media presentasi end-to-end solution yang telah kamu buat

  b. Apabila kamu menggunakan Python untuk mengerjakan tugas ini, kamu perlu mengumpulkan:
    - File Code dengan format .py
    - File Python Notebook dengan format .ipynb
    - File infografis sebagai media presentasi end-to-end solution yang telah kamu buat
4. Kerjakan tugas dan siapkan seluruh file yang perlu kamu kumpulkan dengan mengacu kepada No. 3
5. Gabung ketiga file yang telah kamu selesaikan dalam satu folder ZIP yang diberi judul "[Nama Lengkap]_VIX_ID/X Partners"
6. Submit tugasmu hanya dalam bentuk folder ZIP sesuai ketentuan di No. 5

### BUSINESS UNDERSTANDING

#### 1.1 Objective
Melakukan prediksi nilai Credit Risk berdasarkan *dataset* yang diberikan.

#### 1.2 Description

Dataset yang diberikan memuat data mengenai data *loan* dengan range tahun 2007-2014.

Berdasarkan *dataset*, terdapat 35 total tipe variabel numerik, 22 variabel tipe kategorikal, dan 17 tipe variabel lainnya. Variabel tipe kategori meliputi sebagai berikut:
 - term
 - grade
 - sub_grade
 - emp_title
 - emp_length
 - home_ownership
 - verification_status
 - issue_d
 - loan_status
 - pymnt_plan
 - url
 - desc
 - purpose
 - title
 - zip_code
 - addr_state
 - earliest_cr_line

Solusi yang ditawarkan: **Penulis akan membuat model *machine learning* yang nantinya akan mengidentifikasi status pinjaman berstatus buruk ataupun sebaliknya. Untuk modelnya sendiri, penulis akan menggunakan algoritma *non-parametical*, karena memberikan gambaran model yang terkesan simpel, dan juga mudah untuk dimaparkan kepada orang awam.**

**Penulis memutuskan untuk menggunakan bahasa pemrograman **"Python"**, dengan mempertimbangkan penulisan syntax yang lebih familiar, pemahaman terhadap konsep **Python** yang lebih mendalam dibanding **R**, serta memiliki *learning curve* yang bersifat linear.**

### DATA STORYTELLING
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('loan_data_2007_2014.csv', low_memory = False)

!pip install https://github.com/ydataai/pandas-profiling/archive/master.zip

from pandas_profiling import ProfileReport

profile = ProfileReport(df, title="Loan Data", minimal = True, html={'style':{'full_width':True}}, sort=None)

profile.to_notebook_iframe()

"""### EXPLORATORY DATA ANALYSIS"""

df.head()

df.tail()

df.shape

df.info()

"""Berdasarkan hasil df.info(), sangat banyak kolom yang tidak memiliki nilai sama sekali, terutama beberapa kolom terakhir."""

df.nunique()

df.dtypes

df.describe()

null_values = df.isnull().mean() # Mencari data kolom yang tidak memuat nilai sama sekali

null_values[null_values==1].index # Print list kolom sehingga mempermudah penulis melihat kolom yang tidak memiliki nilai

dropped_df = ['Unnamed: 0', 'annual_inc_joint', 'dti_joint', 'verification_status_joint',
       'open_acc_6m', 'open_il_6m', 'open_il_12m', 'open_il_24m',
       'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m',
       'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl',
       'inq_last_12m']

dataset = df.drop(columns=dropped_df, axis = 1) # Membuat dataframe baru dengan kondisi sudah melakukan drop kolom yang tidak memiliki nilai

dataset.loan_status.value_counts() # Kolom loan_status merupakan kolom yang menjadi target

"""Dikarenakan penulis akan memprediksi apakah status pinjaman jatuhnya buruk/beresiko, maka kita akan membuat klasifikasi status pinjaman dengan catatan:

 - Good Loans = Status -> "Fully Paid"
 - Bad Loans = Status -> "Charged Off", "Late (Jatuh Tempo)."

 Namun ada suatu kondisi seperti "Current", "In Grace Period" yang tidak merujuk kepada 2 catatan diatas. Sehingga untuk kasus seperti ini, penulis menyatakan sebagai status *Loan* yang tidak jelas, dikarenakan kedepannya statusnya bisa Good/Bad Loans. 
"""

good = ['Fully Paid', 'Does not meet the credit policy. Status:Fully Paid'] # Status pinjaman baik
undefined =['Current', 'In Grace Period'] # Status pinjaman tidak jelas

# Penulis melakukan drop terhadap status loan yang dinilai "Undefined" atau tidak jelas
data = dataset[dataset.loan_status.isin(undefined) == False]

data['loan_ending'] = np.where(data['loan_status'].isin(good), 'good', 'bad')

plt.title('Good vs Bad Loan')
sns.barplot(x=data.loan_ending.value_counts().index,y=data.loan_ending.value_counts().values)

# Penulis ingin melakukan pengecekan terhadap kolom dataframe
data.columns

"""Dengan melihat bantuan mengenai penjelasan kolom setiap data dari: https://docs.google.com/spreadsheets/d/1iT1JNOBwU4l616_rnJpo0iny7blZvNBs/edit#gid=1666154857 , Penulis menarik 4 kesimpulan yang penting

1. Kolom yang menjelaskan seputar identitas sang peminjam: 'member_id', 'emp_title', 'emp_length', 'home_ownership', 'annual_inc', 'verification_status', 'zip_code', 'addr_state', 'dti'

2. Kolom yang menjelaskan seputar sifat dasar dari peminjaman: 'id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'application_type'

3. Kolom yang menjelaskan data personal sang peminjam: 'delinq_2yrs', 'earliest_cr_line', 'inq_last_6mths', 'mths_since_last_delinq', 'mths_since_last_record', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'initial_list_status', 'mths_since_last_major_derog', 'acc_now_delinq'

4. Kolom yang menjelaskan status dari sebuah pinjaman: 'issue_d', 'loan_status', 'pymnt_plan', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d'
"""

column_leak = ['issue_d', 'loan_status', 'pymnt_plan', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 
                   'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 
                   'last_pymnt_d', 'next_pymnt_d', 'last_pymnt_amnt']

data.drop(columns=column_leak, axis=1, inplace=True)

data[['loan_amnt','funded_amnt','funded_amnt_inv']].describe()

column_drop = ['funded_amnt', 'funded_amnt_inv', 'id', 'member_id', 'url', 'desc']
dropped_data = data[column_drop]
data.drop(columns=column_drop, axis=1, inplace=True)

data.info()

"""Kedepannya, dataframe "data" yang akan digunakan untuk dilakukan proses *splitting dataset*, visualisasi data, dan pembentukan model *machine learning*.

### DATA VISUALIZATION

#### Visualisasi data dengan beragam kondisi data yang dinilai unik, serta penting.
"""

# Kolom yang menjelaskan data personal sang peminjam (Kesimpulan ke-3)
personal_record = ['mths_since_last_delinq', 'mths_since_last_record', 'mths_since_last_major_derog']
# Ke-3 kolom diatas memiliki banyak null-values
data[personal_record]

# Penulis melakukan asumsi untuk kolom 'mths_since_last_major_derog dengan fungsi np.where 
# Dimana nilai 0 = tidak, 1 = iya
data['major_derogatory'] = np.where(data['mths_since_last_major_derog'].isna(), 0, 1)

# Untuk 2 kolom sisanya, penulis memutuskan untuk melakukan drop kolom 
# karena dinilai sudah dijelaskan pada kolom 'delinq_2yrs' dan kolom ' acc_now_delinq'
drop_col = ['mths_since_last_delinq', 'mths_since_last_record', 'mths_since_last_major_derog']
dropped_data = pd.concat([dropped_data, data[drop_col]], axis = 1)

data.drop(columns=drop_col, axis=1, inplace=True)

columns = ['tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']
print(pd.pivot_table(data, index = 'loan_ending', values = columns))
print(pd.pivot_table(data, index = 'loan_ending', values = columns, aggfunc = np.max))

# Melakukan deskripsi statistik terhadap variabel 'columns' yang memuat 3 kolom data
data[columns].describe()

plt.figure(figsize=(15,8))
sns.kdeplot(data = data[(data['tot_coll_amt'] < 100000) & (data['tot_coll_amt'] > 0)], x='tot_coll_amt', hue='loan_ending')

plt.figure(figsize=(15,8))
sns.kdeplot(data=data[data['tot_cur_bal'] < 800000], x='tot_cur_bal', hue='loan_ending')

plt.figure(figsize=(15,8))
sns.kdeplot(data=data[data['total_rev_hi_lim'] < 250000], x='total_rev_hi_lim', hue='loan_ending')

"""Kesimpulan mengenai kolom 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim' ialah:

1. Kisaran 75% dari kolom 'tot_coll_amt' bernilai = 0
2. Tidak terdapat pemisah yang jelas antara status Good/Bad Loan terhadap nilai kolom diatas.
"""

# Visualisasi data dengan kolom yang memiliki nilai unik dibawah 10
data.nunique()[data.nunique() < 10].sort_values()

"""Dikarenakan kolom 'policy_code' dan 'application_type' hanya memiliki 1 nilai unik saja, penulis memutuskan untuk melakukan drop terhadap ke-2 kolom tersebut."""

data.drop(['policy_code','application_type'], inplace=True, axis = 1)

# Visualisasi data kategori & numerik
num = data.select_dtypes(exclude= 'object')
num.columns

"""Penulis berhasil mendapatkan kolom data yang bertipe numerik"""

categorical = data.select_dtypes(include= 'object')
categorical.columns

"""Penulis berhasil mendapatkan kolom data yang bertipe kategorikal

Penulis menyadari bahwa pada data kategori, terdapat kolom data yang memiliki nilai yang berbeda, seperti 'emp_length', 'earliest_cr_line', dan 'last_credit_pull_d'.
"""

columns = ['emp_length', 'earliest_cr_line', 'last_credit_pull_d']
categorical[columns].head()

data['emp_length'].unique()

emp_map = {
    '< 1 year' : '0',
    '1 year' : '1',
    '2 years' : '2',
    '3 years' : '3',
    '4 years' : '4',
    '5 years' : '5',
    '6 years' : '6',
    '7 years' : '7',
    '8 years' : '8',
    '9 years' : '9',
    '10+ years' : '10'
}
data['emp_length'] = data['emp_length'].map(emp_map).fillna('0').astype(int)
data['emp_length'].unique()

data['earliest_cr_yr'] = pd.to_datetime(data['earliest_cr_line'], format = "%b-%y").dt.year
data['yr_since_last_inq'] = 2016 - pd.to_datetime(data['last_credit_pull_d'], format = "%b-%y").dt.year
data[['emp_length', 'earliest_cr_yr', 'yr_since_last_inq']].describe()

drop_this_columns = ['earliest_cr_line', 'last_credit_pull_d']
dropped_data = pd.concat([dropped_data, data[drop_this_columns]], axis = 1)

numeric_data = data.drop(drop_this_columns, axis=1).select_dtypes(exclude= 'object')
print('numerical data: ', numeric_data.columns)

categorical_data = data.drop(drop_this_columns, axis=1).select_dtypes(include= 'object')
print('categorical data: ', categorical_data.columns)

for i in numeric_data.columns:
    plt.figure(figsize=(10,6))
    plt.hist(numeric_data[i], color = 'yellow')
    plt.title(i)
    plt.show()

plt.figure(figsize=(15 ,8))
sns.heatmap(data=numeric_data.corr(), annot=True)

categorical_data.nunique()

"""Penulis menemukan bahwa pada kolom data 'emp_title', 'title', dan kolom 'zip_code' memiliki *unique* values yang terlalu banyak. Sehingga penulis memutuskan untuk melakukan drop terhadap 3 kolom data tersebut."""

dropped_categorical_data = ['zip_code', 'title', 'emp_title']
dropped_data = pd.concat([dropped_data, data[dropped_categorical_data]], axis = 1)

categorical_data.drop(dropped_categorical_data, axis=1, inplace=True)
categorical_data.columns

dropped_data = pd.concat([dropped_data, data['sub_grade']], axis = 1)
categorical_data.drop('sub_grade', axis = 1, inplace=True)
categorical_data.nunique()

"""### DATA PREPROCESSING

#### Ordinal Encoder
"""

categorical_data['term'] = categorical_data['term'].str.replace(' months', '').astype(int)

categorical_data['grade'].unique()

# Melakukan transformasi terhadap kolom data 'grade'

grade_map = {
    'A' : 1,
    'B' : 2,
    'C' : 3,
    'D' : 4,
    'E' : 5,
    'F' : 6,
    'G' : 7,
}
categorical_data['grade'] = categorical_data['grade'].map(grade_map)

"""#### One Hot Encoding"""

# Source learning: https://towardsdatascience.com/encoding-categorical-variables-one-hot-vs-dummy-encoding-6d5b9c46e2db
dummies_data = ['home_ownership', 'verification_status', 'purpose', 'addr_state', 'initial_list_status']

dummies = pd.get_dummies(categorical_data[dummies_data])
dummies.drop('initial_list_status_w', axis=1, inplace=True)

dummies.head()

dropped_data = pd.concat([dropped_data, categorical_data[dummies_data]], axis = 1)
categorical_data.drop(dummies_data, axis=1, inplace=True)

categorical_data_fused = pd.concat([categorical_data, dummies], axis = 1)

final_data = pd.concat([numeric_data, categorical_data_fused], axis = 1).dropna().reset_index().drop('index', axis = 1)
final_data.head()

"""Salah satu kekurangan dalam menggunakan OneHotEncoding ialah menambah jumlah kolom data secara signifikan, apabila sebuah kolom memiliki nilai unik yang beragam."""

final_data.info()

"""### SPLITTING DATASET INTO TRAINING AND TEST SET"""

X = final_data.drop('loan_ending', axis = 1) # Variabel X mengambil semua fitur, kecuali kolom 'loan_ending', karena kolom tsb menjadi target
y = final_data['loan_ending'] # Variabel y merupakan variabel dependen dan berisi data target 'loan_ending'

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

appended_y_train = np.where(y_train == 'good', 1, 0)
appended_y_test = np.where(y_test == 'good', 1, 0)

"""### MACHINE LEARNING MODEL"""

# Penulis ingin melakukan pengujian performa untuk berbagai algoritma, sehingga dipilih yang memiliki performa terbaik nantinya
# List model yang akan diuji oleh penulis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import cross_val_score
from sklearn import tree
from sklearn.svm import SVC

# Metode evaluasi terhadap model yang diuji
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Algoritma Decision Tree
DT = tree.DecisionTreeClassifier(random_state = 14)
DT.fit(X_train, appended_y_train)
pred_y = DT.predict(X_test)
print(classification_report(appended_y_test, pred_y))

# Algoritma KNN
KNN = KNeighborsClassifier()
KNN.fit(X_train, appended_y_train)
pred_y = KNN.predict(X_test)
print(classification_report(appended_y_test, pred_y))

# Algoritma Random Forest
RF = RandomForestClassifier(random_state = 14)
RF.fit(X_train, appended_y_train)
pred_y = RF.predict(X_test)
print(classification_report(appended_y_test, pred_y))

# Algoritma XGBoost
XGB = XGBClassifier(random_state = 14)
XGB.fit(X_train, appended_y_train)
pred_y = XGB.predict(X_test)
print(classification_report(appended_y_test, pred_y))

# Algoritma Voting Classifier
CLF = VotingClassifier(estimators = [('KNN', KNN),('RF', RF),('XGB', XGB)], voting = 'soft')
CLF.fit(X_train, appended_y_train)
pred_y = CLF.predict(X_test)
print(classification_report(appended_y_test, pred_y))